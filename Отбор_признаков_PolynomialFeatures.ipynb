{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0484c9a-2b2e-4dc9-a803-b62a83b5ce11",
   "metadata": {},
   "source": [
    "Вот несколько стратегий, которые можно использовать для отбора наиболее значимых полиномиальных признаков с использованием CatBoost:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae149ff-d112-4786-8ce0-0e7c9d16347e",
   "metadata": {},
   "source": [
    "**1. Feature Importance из CatBoost:**  \n",
    "Суть: CatBoost предоставляет встроенную возможность оценки важности признаков. После обучения модели можно извлечь эту информацию и использовать ее для отбора наиболее важных полиномиальных признаков.  \n",
    "Преимущества: Простота реализации, встроенный в CatBoost.  \n",
    "Недостатки: Зависит от параметров обучения CatBoost (количество итераций, скорость обучения и т. д.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d37fe-844a-4042-bf7e-c8cd47b24b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# 1. Подготовка данных (замените своими данными)\n",
    "data = {'x1': [1, 2, 3, 4, 5], 'x2': [6, 7, 8, 9, 10], 'y': [11, 12, 13, 14, 15]}\n",
    "df = pd.DataFrame(data)\n",
    "X = df[['x1', 'x2']]\n",
    "y = df['y']\n",
    "\n",
    "# 2. Создание полиномиальных признаков\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)  # degree - степень полинома\n",
    "X_poly = poly.fit_transform(X)\n",
    "feature_names = poly.get_feature_names_out(input_features=['x1', 'x2'])  # Получаем имена признаков\n",
    "X_poly_df = pd.DataFrame(X_poly, columns=feature_names)  # Преобразуем в DataFrame для удобства\n",
    "\n",
    "# 3. Обучение CatBoost\n",
    "model = CatBoostRegressor(iterations=100, learning_rate=0.1, depth=6, loss_function='RMSE', verbose=0)\n",
    "model.fit(X_poly_df, y)\n",
    "\n",
    "# 4. Получение важности признаков\n",
    "feature_importance = model.get_feature_importance()\n",
    "\n",
    "# 5. Создание DataFrame с важностью признаков\n",
    "importance_df = pd.DataFrame({'Feature': X_poly_df.columns, 'Importance': feature_importance})\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# 6. Отбор наиболее важных признаков (например, топ-N)\n",
    "top_n = 3  # Выберите желаемое количество\n",
    "selected_features = importance_df['Feature'][:top_n].tolist()\n",
    "\n",
    "# 7. Фильтрация данных для использования только отобранных признаков\n",
    "X_selected = X_poly_df[selected_features]\n",
    "\n",
    "print(\"Отобранные признаки:\", selected_features)\n",
    "print(X_selected.head())  # Показываем первые несколько строк с отобранными признаками"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1655ef33-db20-4105-ad7f-9dd5cb960549",
   "metadata": {},
   "source": [
    "**2. SelectFromModel (sklearn):**  \n",
    "Суть: Использует модель CatBoost в качестве \"оценщика\" для отбора признаков. Он выбирает признаки на основе заданного порога важности (по умолчанию это среднее значение важности).  \n",
    "Преимущества: Интегрирован с Scikit-learn, позволяет легко экспериментировать с разными порогами отбора.  \n",
    "Недостатки: Порог по умолчанию (среднее значение) может не всегда быть оптимальным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf4e4b6-f14c-457f-92d9-abd5d4afde9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# (Шаги 1-3 как в примере выше)\n",
    "\n",
    "# 4. Использование SelectFromModel\n",
    "sfm = SelectFromModel(model, threshold='mean')  # Можно указать конкретный порог вместо 'mean'\n",
    "sfm.fit(X_poly_df, y)\n",
    "\n",
    "# 5. Получение отобранных признаков\n",
    "selected_features = X_poly_df.columns[sfm.get_support()].tolist()\n",
    "\n",
    "# 6. Фильтрация данных\n",
    "X_selected = X_poly_df[selected_features]\n",
    "\n",
    "print(\"Отобранные признаки:\", selected_features)\n",
    "print(X_selected.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0e870e-a29f-4551-a3d4-b3c949920ce4",
   "metadata": {},
   "source": [
    "**3. Recursive Feature Elimination (RFE):**  \n",
    "Суть: RFE – это жадный алгоритм, который рекурсивно удаляет признаки и обучает модель на оставшихся признаках. Он оценивает, насколько хорошо модель работает с каждым набором признаков, и выбирает лучший набор.  \n",
    "Преимущества: Хорошо работает, когда признаки сильно коррелированы. Позволяет напрямую указать желаемое количество признаков.  \n",
    "Недостатки: Вычислительно дорогой, особенно для больших наборов данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df782c29-1e97-4ea4-ba6b-68f8e25d681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# (Шаги 1-3 как в примере выше)\n",
    "\n",
    "# 4. Использование RFE\n",
    "rfe = RFE(model, n_features_to_select=3)  # Укажите желаемое количество признаков\n",
    "rfe.fit(X_poly_df, y)\n",
    "\n",
    "# 5. Получение отобранных признаков\n",
    "selected_features = X_poly_df.columns[rfe.support_].tolist()\n",
    "\n",
    "# 6. Фильтрация данных\n",
    "X_selected = X_poly_df[selected_features]\n",
    "\n",
    "print(\"Отобранные признаки:\", selected_features)\n",
    "print(X_selected.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894bb962-c4df-41d9-939c-06c96716421c",
   "metadata": {},
   "source": [
    "**Рекурсивное исключение признаков с кросс-валидацией (RFECV):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0183039-a333-4389-ac9e-93df354e7e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Создадим синтетический набор данных\n",
    "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Инициализируем модель\n",
    "model = LinearRegression()\n",
    "\n",
    "# Инициализируем RFECV с кросс-валидацией\n",
    "rfecv = RFECV(estimator=model, step=1, cv=KFold(n_splits=5), scoring='neg_mean_squared_error')\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(\"Оптимальное количество признаков : %d\" % rfecv.n_features_)\n",
    "print(\"Выбранные признаки: %s\" % rfecv.support_)\n",
    "print(\"Рейтинг признаков (1 - лучший): %s\" % rfecv.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a6c3ab-5180-4726-997b-1628485d4c92",
   "metadata": {},
   "source": [
    "**4. Отбор на основе перестановочной важности (permutation importance):**  \n",
    "Суть: Перетасовываем значения каждого признака по очереди и смотрим, насколько сильно ухудшается производительность модели. Если ухудшение большое, значит признак важный.  \n",
    "Преимущества: Позволяет оценить, насколько признаки _действительно_ важны для модели, а не только то, как часто они использовались в процессе обучения.  \n",
    "Недостатки: Вычислительно более затратный, чем просто получение важности признаков из CatBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ea7636-d596-4511-bde5-04a81b5fa4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "\n",
    "# (Шаги 1-3 как в примере выше)\n",
    "\n",
    "# 4. Вычисление перестановочной важности\n",
    "permutation_importance = eli5.PermutationImportance(model, random_state=42)\n",
    "permutation_importance.fit(X_poly_df, y)\n",
    "\n",
    "# 5. Отображение результатов\n",
    "eli5.show_weights(permutation_importance, feature_names=X_poly_df.columns.tolist())\n",
    "\n",
    "# На основе вывода eli5.show_weights() выберите признаки с высокой важностью и отфильтруйте данные\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0dd791-fc99-4468-8763-2b9eea148a14",
   "metadata": {},
   "source": [
    "**5. Отбор признаков на основе перестановок (Permutation Importance):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab9e5f3-1beb-4c6d-a27d-482422ac1a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "import pandas as pd\n",
    "\n",
    "# Создадим синтетический набор данных\n",
    "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
    "X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])  # Для получения названий колонок\n",
    "\n",
    "# Разделим данные на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Инициализируем и обучаем модель\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Вычисляем важность признаков на основе перестановок\n",
    "result = permutation_importance(model, X_test, y_test, scoring='neg_mean_squared_error', n_repeats=10, random_state=42)\n",
    "\n",
    "# Сортируем признаки по важности\n",
    "sorted_idx = result.importances_mean.argsort()[::-1]\n",
    "\n",
    "print(\"Важность признаков (перестановки):\")\n",
    "for i in sorted_idx:\n",
    "    print(f\"{X.columns[i]}: {result.importances_mean[i]:.4f} +/- {result.importances_std[i]:.4f}\")\n",
    "\n",
    "# Отбор признаков на основе порога (например, отбираем только положительные значения важности)\n",
    "selected_features = X.columns[result.importances_mean > 0]\n",
    "print(\"\\nВыбранные признаки:\", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f56a8c-8158-4eb4-9bfa-965a90d35224",
   "metadata": {},
   "source": [
    "**6. Последовательный поиск (Sequential Feature Selection, SFS):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9a3954-0293-4908-a407-376034901e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "# Создадим синтетический набор данных\n",
    "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Разделим данные на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Sequential Forward Selection\n",
    "sfs = SFS(LinearRegression(),\n",
    "          k_features='best',  # Автоматически выбирает лучшее количество признаков на основе кросс-валидации\n",
    "          forward=True,\n",
    "          floating=False,\n",
    "          scoring='neg_mean_squared_error',\n",
    "          cv=5)\n",
    "\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "print(\"Последовательный прямой выбор (SFS):\")\n",
    "print(\"Лучшие признаки:\", sfs.k_feature_names_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fefcfd8-3a46-4bba-bec6-04331d54827b",
   "metadata": {},
   "source": [
    "**7. Метод отбора признаков на основе генетического алгоритма:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ff4a0d-9be3-45d2-a9e1-50a42573a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Создадим синтетический набор данных\n",
    "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Разделим данные на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Инициализируем TPOTRegressor (настраиваем на отбор признаков с LinearRegression)\n",
    "tpot = TPOTRegressor(\n",
    "    generations=5,\n",
    "    population_size=20,\n",
    "    offspring_size=None,\n",
    "    mutation_rate=0.9,\n",
    "    crossover_rate=0.1,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    "    verbosity=2,\n",
    "    n_jobs=-1,\n",
    "    config_dict={'sklearn.linear_model.LinearRegression': {}},\n",
    "    template='Selector-Transformer-Regressor',\n",
    ")\n",
    "\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c13fb51-30fc-4b7c-b42d-20556eafb708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Создадим синтетический набор данных\n",
    "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Определяем конфигурацию TPOT, включающую CatBoost\n",
    "tpot_config = {\n",
    "        'catboost.CatBoostRegressor': {\n",
    "        'iterations': [100, 200, 300],  #  Примерные значения, настройте под свою задачу\n",
    "        'learning_rate': [0.01, 0.03, 0.1], # Примерные значения\n",
    "        'depth': [4, 6, 8], # Примерные значения\n",
    "        'l2_leaf_reg': [1, 3, 5], # Примерные значения\n",
    "        'random_state': [42],\n",
    "        'verbose': [False]  # Отключаем вывод информации от CatBoost\n",
    "    },\n",
    "    # Добавьте другие модели и параметры, если хотите, чтобы TPOT их тоже рассматривал\n",
    "}\n",
    "\n",
    "\n",
    "# Инициализируем TPOTRegressor с конфигурацией\n",
    "tpot = TPOTRegressor(\n",
    "    generations=5,  #  Увеличьте для более тщательного поиска\n",
    "    population_size=20, # Увеличьте для более тщательного поиска\n",
    "    offspring_size=None,\n",
    "    mutation_rate=0.9,\n",
    "    crossover_rate=0.1,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    "    verbosity=2,\n",
    "    n_jobs=-1,\n",
    "    config_dict=tpot_config,  # Используем нашу конфигурацию с CatBoost\n",
    "    template='Selector-Transformer-Regressor', # Убедитесь, что включены этапы отбора признаков\n",
    ")\n",
    "\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_catboost_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc98a68-0d7e-4992-a76f-331726df7c80",
   "metadata": {},
   "source": [
    "К сожалению, TPOT не предоставляет простого способа напрямую получить список отобранных признаков. Однако вы можете извлечь эту информацию из экспортированного пайплайна (файла .py, созданного с помощью tpot.export())."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b7444f-dce2-4784-868b-2fe267fee273",
   "metadata": {},
   "source": [
    "**Рекомендации и best practices:**\n",
    "\n",
    "**•  Кросс-валидация:** Используйте кросс-валидацию при отборе признаков, чтобы убедиться, что выбранные признаки хорошо обобщаются на новых данных. Это очень важно, чтобы избежать переобучения! Например, можно встроить процесс отбора признаков в конвейер (Pipeline) scikit-learn и использовать cross_val_score.\n",
    "\n",
    "**•  Комбинация методов:** Попробуйте комбинировать разные методы отбора признаков. Например, можно сначала использовать SelectFromModel с мягким порогом, а затем применить RFE для дальнейшего сокращения числа признаков.\n",
    "\n",
    "**•  Визуализация:** Визуализируйте важность признаков (например, с помощью столбчатой диаграммы), чтобы лучше понять, какие признаки наиболее важны.\n",
    "\n",
    "**•  Экспериментируйте с параметрами:** Попробуйте разные значения степени полинома (degree), количества итераций CatBoost, порога отбора признаков и т. д.\n",
    "\n",
    "**•  Ручной отбор:** Иногда полезно провести ручной анализ признаков на основе вашего понимания предметной области. Например, если вы знаете, что определенные комбинации признаков имеют смысл в контексте задачи, вы можете сохранить их, даже если они не очень важны по мнению модели.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed7cfba-4952-4c97-bd46-08fa1037fa0c",
   "metadata": {},
   "source": [
    "**Пример использования Feature Importance с кросс-валидацией:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d8cab-3946-4bee-b5da-b51527a7394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Класс для отбора признаков на основе важности CatBoost\n",
    "class CatBoostFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model, top_n=5):\n",
    "        self.model = model\n",
    "        self.top_n = top_n\n",
    "        self.selected_features = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.model.fit(X, y)\n",
    "        feature_importance = self.model.get_feature_importance()\n",
    "        importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})\n",
    "        importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "        self.selected_features = importance_df['Feature'][:self.top_n].tolist()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.selected_features]\n",
    "\n",
    "\n",
    "# 1. Подготовка данных (замените своими данными)\n",
    "data = {'x1': [1, 2, 3, 4, 5], 'x2': [6, 7, 8, 9, 10], 'y': [11, 12, 13, 14, 15]}\n",
    "df = pd.DataFrame(data)\n",
    "X = df[['x1', 'x2']]\n",
    "y = df['y']\n",
    "\n",
    "\n",
    "# 2. Создание пайплайна\n",
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('selector', CatBoostFeatureSelector(CatBoostRegressor(iterations=100, learning_rate=0.1, depth=6, loss_function='RMSE', verbose=0), top_n=3)),\n",
    "    ('model', CatBoostRegressor(iterations=100, learning_rate=0.1, depth=6, loss_function='RMSE', verbose=0)) # Обучаем модель после отбора признаков\n",
    "])\n",
    "\n",
    "\n",
    "# 3. Кросс-валидация\n",
    "scores = cross_val_score(pipeline, X, y, cv=5, scoring='neg_mean_squared_error') # Или другая метрика\n",
    "print(\"Кросс-валидация MSE:\", -scores.mean()) #  Инвертируем знак, так как cross_val_score максимизирует функцию, а MSE нужно минимизировать\n",
    "\n",
    "# 4. Обучение пайплайна на всех данных (после кросс-валидации)\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# 5.  Получение отобранных признаков\n",
    "selected_features = pipeline.named_steps['selector'].selected_features\n",
    "print(\"Отобранные признаки:\", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4f00c9-8c97-46af-b507-545cd1e911bb",
   "metadata": {},
   "source": [
    "**PolynomialFeatures**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be76ada-a8fe-4a92-b0f8-3903b6982e78",
   "metadata": {},
   "source": [
    "1. Квадратичные члены для OHE-признаков:\n",
    "\n",
    "Вы совершенно верно заметили, что возведение в квадрат OHE-признаков не имеет смысла. Поскольку OHE-признаки принимают значения 0 или 1, то 0^2 = 0 и 1^2 = 1. То есть, квадратичные члены не добавят никакой новой информации. Сами по себе квадраты OHE-признаков бесполезны.\n",
    "\n",
    "2. Произведения OHE-признаков и числового признака:\n",
    "\n",
    "Произведения между OHE-признаками и числовым признаком (в вашем случае, площади) имеют смысл и часто полезны. Они создают эффект взаимодействия между категориальным и числовым признаками.\n",
    "\n",
    "•  Интерпретация: Произведение OHE-признака \"отделка_улучшенная\" и площади можно интерпретировать как \"площадь, только для квартир с улучшенной отделкой\". По сути, это позволяет модели учитывать разный наклон зависимости целевой переменной от площади для каждой категории отделки. Если коэффициент при этом произведении будет значимым, это говорит о том, что зависимость целевой переменной от площади различна для квартир с улучшенной отделкой по сравнению с другими.\n",
    "\n",
    "•  Нет проблем с нулями и единицами: Нули и единицы в OHE-признаках в данном случае играют роль \"выключателя\" и \"включателя\". Когда OHE-признак равен 0, произведение равно 0, и, следовательно, вклад площади для этой категории отделки не учитывается. Когда OHE-признак равен 1, произведение равно площади, и, следовательно, вклад площади для этой категории отделки учитывается в полном объеме. Это ровно то, что нам нужно для моделирования взаимодействия.\n",
    "\n",
    "Что делать в вашем случае?\n",
    "\n",
    "Учитывая все вышесказанное, вот как можно поступить при создании полиномиальных признаков в вашей ситуации:\n",
    "\n",
    "1. Не создавайте квадратичные члены для OHE-признаков. Просто пропустите их.\n",
    "\n",
    "2. Создавайте произведения между OHE-признаками и числовым признаком (площадью). Это позволит модели учитывать разный эффект площади для каждой категории отделки.\n",
    "\n",
    "3. Создавайте квадратичные и кубические члены для площади. Важно помнить про регуляризацию и масштабирование.\n",
    "\n",
    "4. Подумайте о создании попарных произведений OHE признаков. Их стоит создавать только в том случае, если это имеет смысл с точки зрения предметной области. Попарные произведения создадут новые категориальные признаки, моделирующие совместное присутствие двух категорий. Однако, если ваши категории взаимоисключающие (как в случае с отделкой, где квартира может иметь только один тип отделки), то попарные произведения всегда будут равны нулю."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4756e0b-da54-492f-9b13-3ae025049bb4",
   "metadata": {},
   "source": [
    "1. Попарные произведения OHE-признаков одного фактора (состояние отделки):\n",
    "\n",
    "•   Да, попарные произведения OHE-признаков одного фактора, если фактор взаимоисключающий (как \"состояние отделки\"), всегда будут равны 0.  Это происходит потому, что в каждой строке данных только один из OHE-признаков может быть равен 1, а все остальные равны 0.  Поэтому, произведение любых двух OHE-признаков из одного и того же фактора всегда будет равно 0.\n",
    "•   Да, такие признаки (попарные произведения OHE-признаков одного взаимоисключающего фактора) необходимо исключить, так как они не несут никакой полезной информации.  Они только увеличивают размерность пространства признаков и могут запутать модель.\n",
    "\n",
    "2. Попарные произведения OHE-признаков разных факторов (состояние отделки и местоположение (район)):\n",
    "\n",
    "•   Да, попарные произведения OHE-признаков разных факторов *допускаются* и часто полезны.  Они моделируют *совместное влияние* или *взаимодействие* этих факторов на целевую переменную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d8171f-4acd-4ebc-abcb-902902f64f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Пример данных\n",
    "data = {'отделка': ['стандарт', 'улучшенный', 'без отделки', 'стандарт', 'улучшенный'],\n",
    "        'площадь': [50, 70, 40, 60, 80],\n",
    "        'цена': [100, 150, 80, 120, 170]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 1. OHE для категориального признака\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False) # sparse_output=False для удобства работы\n",
    "ohe.fit(df[['отделка']])\n",
    "ohe_features = ohe.transform(df[['отделка']])\n",
    "ohe_feature_names = ohe.get_feature_names_out(['отделка']) # ['отделка_без отделки', 'отделка_стандарт', 'отделка_улучшенный']\n",
    "ohe_df = pd.DataFrame(ohe_features, columns=ohe_feature_names)\n",
    "df = pd.concat([df, ohe_df], axis=1)\n",
    "df = df.drop('отделка', axis=1) # Удаляем исходный столбец 'отделка'\n",
    "\n",
    "# 2. Полиномиальные признаки\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False) # interaction_only=False для квадратов чисел\n",
    "\n",
    "#  Выбираем признаки для полиномиальных преобразований (площадь и OHE признаки)\n",
    "features_for_poly = ['площадь'] + list(ohe_feature_names)\n",
    "X = df[features_for_poly]\n",
    "\n",
    "#  Преобразуем данные, создавая полиномиальные признаки\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "#  Создаем имена для новых признаков.  Это немного сложно, потому что PolynomialFeatures создает много комбинаций\n",
    "poly_feature_names = poly.get_feature_names_out(features_for_poly)\n",
    "poly_df = pd.DataFrame(X_poly, columns=poly_feature_names)\n",
    "\n",
    "\n",
    "#  Удаляем квадраты OHE признаков (они неинформативны, как мы обсуждали)\n",
    "for feature_name in ohe_feature_names:\n",
    "    square_feature_name = f\"{feature_name}^2\"\n",
    "    if square_feature_name in poly_df.columns:\n",
    "        poly_df = poly_df.drop(square_feature_name, axis=1)\n",
    "\n",
    "#  Соединяем полиномиальные признаки с исходными\n",
    "df = pd.concat([df, poly_df], axis=1)\n",
    "\n",
    "# Теперь у вас есть DataFrame `df` со всеми нужными признаками\n",
    "# Добавляем целевую переменную\n",
    "y = df['цена']\n",
    "X = df.drop('цена', axis=1)\n",
    "\n",
    "#  Разделяем на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Создаем и обучаем модель (пример с линейной регрессией и масштабированием)\n",
    "model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('linear_regression', LinearRegression())\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Оценка модели\n",
    "score = model.score(X_test, y_test)\n",
    "print(f\"R^2 score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d446e8db-f50d-4c34-847e-981bfdec8e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Пример данных (добавляем признак \"район\")\n",
    "data = {'отделка': ['стандарт', 'улучшенный', 'без отделки', 'стандарт', 'улучшенный'],\n",
    "        'район': ['центр', 'спальный', 'центр', 'спальный', 'центр'],\n",
    "        'площадь': [50, 70, 40, 60, 80],\n",
    "        'цена': [100, 150, 80, 120, 170]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 1. OHE для категориальных признаков\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "ohe.fit(df[['отделка', 'район']])\n",
    "ohe_features = ohe.transform(df[['отделка', 'район']])\n",
    "ohe_feature_names = ohe.get_feature_names_out(['отделка', 'район'])\n",
    "ohe_df = pd.DataFrame(ohe_features, columns=ohe_feature_names)\n",
    "df = pd.concat([df, ohe_df], axis=1)\n",
    "df = df.drop(['отделка', 'район'], axis=1)\n",
    "\n",
    "# 2. Создаем имена OHE признаков для \"отделки\" и \"района\"\n",
    "ohe_otdelka_names = [name for name in ohe_feature_names if 'отделка_' in name]\n",
    "ohe_raion_names = [name for name in ohe_feature_names if 'район_' in name]\n",
    "\n",
    "# 3. Создаем попарные произведения OHE-признаков разных факторов (\"отделка\" и \"район\")\n",
    "for otdelka_name in ohe_otdelka_names:\n",
    "    for raion_name in ohe_raion_names:\n",
    "        df[f'{otdelka_name}_x_{raion_name}'] = df[otdelka_name] * df[raion_name]\n",
    "\n",
    "# 4. Полиномиальные признаки для площади (квадратичные, кубические и т.д., если нужно)\n",
    "df['площадь_2'] = df['площадь']**2\n",
    "# df['площадь_3'] = df['площадь']**3 # и т.д.\n",
    "\n",
    "#  Удаляем исходные признаки после OHE, если больше не нужны\n",
    "# X = df.drop('цена', axis=1)\n",
    "\n",
    "# Создаем и обучаем модель (пример с линейной регрессией и масштабированием)\n",
    "#  Разделяем на обучающую и тестовую выборки\n",
    "y = df['цена']\n",
    "X = df.drop('цена', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('linear_regression', LinearRegression())\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Оценка модели\n",
    "score = model.score(X_test, y_test)\n",
    "print(f\"R^2 score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f483d78f-7454-4e06-bc52-e93d18e3f3b2",
   "metadata": {},
   "source": [
    "Этот параметр определяет, нужно ли создавать только взаимодействия между признаками, или также создавать степени отдельных признаков.  \n",
    "•  interaction_only=True: Если установлено значение True, то будут создаваться только признаки, представляющие собой произведения различных входных признаков. Степени отдельных признаков (например, x1^2, x2^3) создаваться не будут. Это полезно, когда вас интересуют только взаимодействия между признаками, а не их нелинейные преобразования.  \n",
    "•  interaction_only=False (значение по умолчанию): Если установлено значение False, то будут создаваться как взаимодействия между признаками, так и степени отдельных признаков. То есть, вы получите все возможные полиномиальные комбинации признаков до заданной степени. Это полезно, когда вы хотите, чтобы модель могла учитывать как взаимодействия, так и нелинейные эффекты отдельных признаков.\n",
    "\n",
    "Этот параметр определяет, нужно ли включать в выходные данные столбец \"смещения\" (bias, intercept).  \n",
    "•  include_bias=True (значение по умолчанию, если не указано другое): Если установлено значение True, то в выходные данные будет добавлен столбец, состоящий из одних единиц. Этот столбец представляет собой \"фиктивный\" признак, который позволяет модели иметь свободный член (intercept) в линейной комбинации признаков. Многие модели (например, LinearRegression) автоматически добавляют свободный член, поэтому этот столбец часто не нужен и может быть даже вреден (например, вызвать мультиколлинеарность).  \n",
    "•  include_bias=False: Если установлено значение False, то столбец смещения добавляться не будет. Это полезно, если вы уверены, что ваша модель уже учитывает смещение (например, если вы используете модель, которая автоматически добавляет свободный член, или если вы явно добавляете столбец с единицами в свои данные)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48a28b8-449f-4296-aa04-71cd26709f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b5b04d-e7c7-49f6-a4d0-c6e08e4a10a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
