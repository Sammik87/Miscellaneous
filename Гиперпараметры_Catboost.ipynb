{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6116ef9-9b1a-4aaf-8d9f-68874ce08923",
   "metadata": {},
   "source": [
    "#### GPT4-o-mini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1f42fe-3f8c-40c0-a0c0-11cadab1b511",
   "metadata": {},
   "source": [
    "Основные гиперпараметры CatBoost для улучшения качества моделей оценки стоимости квартир (в порядке убывания важности, как я считаю, основываясь на опыте применения CatBoost):\n",
    "\n",
    "1. learning_rate (или eta):\n",
    "  •  Влияние: Регулирует величину шага при обновлении весов деревьев. Меньшие значения требуют большего количества итераций, но часто приводят к более точным и стабильным результатам.\n",
    "  •  Подбор: Начните с значений в диапазоне [0.01, 0.1]. Используйте GridSearchCV или RandomizedSearchCV для поиска оптимального значения в этом диапазоне. Важно помнить, что learning_rate тесно связан с n_estimators.\n",
    "  •  Рекомендация: При уменьшении learning_rate обычно необходимо увеличивать n_estimators. Пример: learning_rate=0.01, n_estimators=1000 может быть лучше, чем learning_rate=0.1, n_estimators=100.\n",
    "\n",
    "2. n_estimators (или iterations):\n",
    "  •  Влияние: Определяет количество деревьев в ансамбле. Слишком мало деревьев - недообучение, слишком много - переобучение (особенно без регуляризации).\n",
    "  •  Подбор: Используйте early stopping (early_stopping_rounds) вместе с большим начальным значением n_estimators (например, 1000 или 2000). Early stopping остановит обучение, когда качество на валидационном наборе перестанет улучшаться.\n",
    "  •  Рекомендация: Всегда используйте early stopping! Это критически важно для предотвращения переобучения и выбора оптимального количества деревьев. early_stopping_rounds обычно устанавливают в значениях от 50 до 200.\n",
    "\n",
    "3. max_depth:\n",
    "  •  Влияние: Ограничивает глубину каждого дерева. Слишком большая глубина приводит к переобучению, слишком маленькая - к недообучению.\n",
    "  •  Подбор: Начните с диапазона [4, 10]. Оптимальное значение зависит от сложности ваших данных.\n",
    "  •  Рекомендация: Для данных о недвижимости, где есть сложные зависимости, max_depth в районе 6-8 часто показывает хорошие результаты.\n",
    "\n",
    "4. l2_leaf_reg (или reg_lambda):\n",
    "  •  Влияние: L2 регуляризация. Добавляет штраф к функции потерь за большие значения весов листьев деревьев. Предотвращает переобучение.\n",
    "  •  Подбор: Начните с диапазона [1, 10].\n",
    "  •  Рекомендация: Это один из самых важных параметров регуляризации. Не пренебрегайте им.\n",
    "\n",
    "5. loss_function:\n",
    "  •  Влияние: Определяет функцию потерь, которую нужно минимизировать.\n",
    "  •  Подбор: Для задач регрессии (оценка стоимости) наиболее часто используются:\n",
    "    *  'RMSE': Root Mean Squared Error (корень из среднеквадратичной ошибки) - чувствителен к выбросам.\n",
    "    *  'MAE': Mean Absolute Error (средняя абсолютная ошибка) - более устойчив к выбросам.\n",
    "    *  'Huber': Комбинация RMSE и MAE, менее чувствителен к выбросам, чем RMSE.\n",
    "    *  'Poisson': Подходит, если целевая переменная имеет пуассоновское распределение (например, количество продаж).\n",
    "  •  Рекомендация: Начните с RMSE или MAE. Если в данных много выбросов, попробуйте Huber.\n",
    "\n",
    "6. random_strength:\n",
    "  •  Влияние: Добавляет случайность при выборе разбиений в деревьях. Увеличивает устойчивость модели к переобучению.\n",
    "  •  Подбор: Обычно в диапазоне [0.1, 2].\n",
    "  •  Рекомендация: Помогает, когда модель сильно переобучается.\n",
    "\n",
    "7. border_count:\n",
    "  •  Влияние: Определяет количество бинарных разбиений для каждого признака. Большее значение позволяет находить более точные разбиени\n",
    "я, но увеличивает время обучения.\n",
    "  •  Подбор: Обычно 32, 64, 128, 254.\n",
    "  •  Рекомендация: Не всегда необходимо менять. Увеличение может помочь, если у вас много числовых признаков.\n",
    "\n",
    "8. feature_fraction (или colsample_bylevel):\n",
    "  •  Влияние: Определяет долю признаков, которые используются при построении каждого дерева. Помогает бороться с переобучением и улучшает обобщающую способность.\n",
    "  •  Подбор: Диапазон [0.6, 0.9].\n",
    "  •  Рекомендация: Используйте, если у вас очень много признаков.\n",
    "\n",
    "9. task_type:\n",
    "  * Влияние: Определяет, будет ли CatBoost использовать GPU или CPU.\n",
    "  * Подбор: Если у вас есть GPU, установите task_type='GPU' для значительного ускорения обучения.\n",
    "  * Рекомендация: Обязательно используйте GPU, если он доступен."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc11513-94e5-4b7e-8e2c-79faa2e33df2",
   "metadata": {},
   "source": [
    "#### GPT4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4117346-639e-4cd0-aebe-7dce6ea92c2d",
   "metadata": {},
   "source": [
    "### **Основные гиперпараметры, которые стоит учитывать:**  \n",
    "\n",
    "1. **`max_depth`** – Определяет глубину деревьев.  \n",
    "   - Чем больше, тем сложнее модель (больше переобучение).  \n",
    "   - Оптимальный диапазон: **6–10**.  \n",
    "\n",
    "2. **`learning_rate`** – Скорость обучения модели.  \n",
    "   - Маленькие значения улучшают стабильность, но требуют больше итераций.  \n",
    "   - Оптимальный диапазон: **0.01–0.1**.  \n",
    "\n",
    "3. **`n_estimators`** – Количество деревьев (итераций).  \n",
    "   - Чем выше, тем лучше модель подстраивается под данные.  \n",
    "   - Обычно балансируют с `learning_rate`: **500–5000** (чем меньше `learning_rate`, тем больше `n_estimators`).  \n",
    "\n",
    "4. **`early_stopping_rounds`** – Прерывает обучение, если качество ухудшается.  \n",
    "   - Ты правильно ставишь **10**, но если данных много, можно увеличить до **20–50**.  \n",
    "\n",
    "5. **`l2_leaf_reg`** – Регуляризация L2 для контроля переобучения.  \n",
    "   - Оптимальный диапазон: **1–10** (по умолчанию **3**).  \n",
    "\n",
    "6. **`depth`** – Альтернативный вариант `max_depth`, влияет на сложность деревьев.  \n",
    "   - Обычно совпадает с `max_depth`, но можно тестировать **6–10**.  \n",
    "\n",
    "7. **`boosting_type`** – Алгоритм построения бустинга.  \n",
    "   - **Plain** – стандартный бустинг (по умолчанию).  \n",
    "   - **Ordered** – может быть полезен, если данных мало.  \n",
    "\n",
    "8. **`bagging_temperature`** – Контроль случайности в выборе объектов.  \n",
    "   - **0–1**: при **1** больше случайности, при **0** модель стабильнее.  \n",
    "   - Для твоей задачи можно тестировать **0.5–1**.  \n",
    "\n",
    "9. **`random_strength`** – Контроль случайности разбиения деревьев.  \n",
    "   - Оптимальный диапазон: **1–10** (обычно **1**).  \n",
    "\n",
    "10. **`custom_loss`** – Можно использовать `['MAE', 'MAPE']`, так как ты их оцениваешь.  \n",
    "\n",
    "### **Дополнительные параметры, если данных много:**  \n",
    "\n",
    "- **`grow_policy`** – Как строятся деревья:  \n",
    "  - `SymmetricTree` (по умолчанию) – сбалансированное дерево.  \n",
    "  - `Depthwise` – полезно при глубине >10.  \n",
    "  - `Lossguide` – лучше при больших данных.  \n",
    "\n",
    "- **`colsample_bylevel`** – Доля признаков при построении дерева (0.5–1).  \n",
    "\n",
    "- **`boost_from_average`** – Ускоряет обучение, можно **True**.  \n",
    "\n",
    "### **Твой текущий процесс – хороший старт, но можно улучшить:**  \n",
    "- Добавь подбор `l2_leaf_reg`, `bagging_temperature`, `random_strength`.  \n",
    "- Проверь `boosting_type='Ordered'`, если данных мало.  \n",
    "- `custom_loss=['MAE', 'MAPE']` для лучшего контроля качества.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ae4844-900f-4ea5-820d-a6b3c5be5d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Определяем модель CatBoost (без параметров, они будут подбираться)\n",
    "model = CatBoostRegressor(loss_function='MAE', verbose=0, random_state=42)\n",
    "\n",
    "# Сетка гиперпараметров\n",
    "param_grid = {\n",
    "    'max_depth': [6, 8, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'l2_leaf_reg': [1, 3, 5, 7],\n",
    "    'bagging_temperature': [0, 0.5, 1]\n",
    "}\n",
    "\n",
    "# Запускаем GridSearchCV\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Выводим лучшие параметры\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "print(\"Лучший MAE:\", -grid_search.best_score_)\n",
    "\n",
    "# Финальная модель с лучшими параметрами\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdefa2e2-75c9-43b6-b074-c9375d8672d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Базовая модель CatBoost\n",
    "model = CatBoostRegressor(loss_function='MAE', verbose=0, random_state=42)\n",
    "\n",
    "# Определяем диапазоны гиперпараметров\n",
    "param_dist = {\n",
    "    'max_depth': np.arange(4, 11, 1),  # От 4 до 10\n",
    "    'learning_rate': np.linspace(0.01, 0.3, 10),  # 10 значений от 0.01 до 0.3\n",
    "    'l2_leaf_reg': np.linspace(1, 10, 10),  # 10 значений от 1 до 10\n",
    "    'bagging_temperature': np.linspace(0, 1, 5),  # 5 значений от 0 до 1\n",
    "    'iterations': np.arange(500, 5000, 500)  # От 500 до 5000 с шагом 500\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV - тестируем 30 случайных комбинаций (n_iter=30)\n",
    "random_search = RandomizedSearchCV(\n",
    "    model, param_distributions=param_dist, n_iter=30, \n",
    "    cv=5, scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42\n",
    ")\n",
    "\n",
    "# Обучаем подбор параметров\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Выводим лучшие параметры\n",
    "print(\"Лучшие параметры:\", random_search.best_params_)\n",
    "print(\"Лучший MAE:\", -random_search.best_score_)\n",
    "\n",
    "# Финальная модель с лучшими параметрами\n",
    "best_model = random_search.best_estimator_\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
