{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1875ff7-4fe3-4b03-a7f6-9ace753a3049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import spacy\n",
    "python -m spacy download ru_core_news_md\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymystem3 import Mystem  # Лемматизация (опционально)\n",
    "from natasha import Doc, Segmenter, MorphVocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508f4426-bb9a-4430-a50e-9b1c8ff370ac",
   "metadata": {},
   "source": [
    "### **1. Нужно ли обрабатывать текст перед подачей его в `text_features`?**\n",
    "\n",
    "- **CatBoost** может работать с текстом \"как есть\", так как внутри уже встроены базовые механизмы обработки текстов. Однако, это не значит, что текстовая предобработка бесполезна — она может значительно улучшить качество модели.\n",
    "\n",
    "- **Рекомендация**: базовую предобработку текста все же стоит делать, особенно если текстовые данные \"шумные\" (содержат опечатки, специальные символы, избыточные пробелы и т.д.).\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Какую предобработку текста делает CatBoost внутри?**\n",
    "\n",
    "CatBoost применяет **встроенные методы обработки текстовых данных**, которые включают:\n",
    "1. **Применение токенизации**:\n",
    "   - Текст разбивается на отдельные слова (токены). Это может быть сделано на уровне пробелов, знаков препинания или других символов.\n",
    "2. **Построение векторов на основе n-грамм**:\n",
    "   - CatBoost строит числовые представления текста, основываясь на n-граммах (последовательностях из 1–3 слов).\n",
    "3. **TF-IDF**:\n",
    "   - Вычисляет важность слов, учитывая частотность слов в тексте конкретного объявления и в корпусе текстов.\n",
    "4. **Многослойное поблочное взвешивание**:\n",
    "   - CatBoost создает эмбеддинги текста на основе многоуровневого подхода, обучая веса во время обучения модели.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Какие этапы предобработки текста можно выполнить для улучшения качества модели?**\n",
    "\n",
    "Хотя CatBoost может обрабатывать текст самостоятельно, **предварительная обработка текста** может улучшить его качество. Вот что можно сделать:\n",
    "\n",
    "#### **(а) Удаление шума**\n",
    "   - Удалите ненужные символы: HTML-теги, спецсимволы, эмодзи, лишние пробелы.\n",
    "   - Пример: \"Квартира!!! (100%) в центре\" → \"Квартира в центре\"\n",
    "\n",
    "#### **(б) Приведение к одному регистру**\n",
    "   - Приведение всех символов к нижнему регистру для унификации.\n",
    "   - Пример: \"КВАРТИРА\" → \"квартира\"\n",
    "\n",
    "#### **(в) Токенизация**\n",
    "   - Разделение текста на отдельные слова или токены.\n",
    "   - Можно использовать библиотеки, такие как `nltk`, `spaCy`, или `razdel`.\n",
    "\n",
    "#### **(г) Лемматизация или стемминг**\n",
    "   - Приведение слов к их базовой форме.\n",
    "   - Пример: \"продается\", \"продажа\" → \"продать\"\n",
    "\n",
    "#### **(д) Удаление стоп-слов**\n",
    "   - Удаление часто встречающихся, но малозначимых слов (`и`, `в`, `на` и т.д.).\n",
    "   - Пример: \"квартира в центре города\" → \"квартира центр город\"\n",
    "\n",
    "#### **(е) Словарь синонимов**\n",
    "   - Объедините синонимы для унификации текстов.\n",
    "   - Пример: \"шикарная\", \"роскошная\" → \"элитная\"\n",
    "\n",
    "#### **(ж) Выделение ключевых слов**\n",
    "   - Если тексты очень большие, можно выделить ключевые слова с помощью TF-IDF или библиотеки `TextRank`.\n",
    "\n",
    "#### **(з) Удаление дубликатов**\n",
    "   - Проверьте, нет ли объявлений с идентичным текстом.\n",
    "\n",
    "#### **(и) Сокращение размера текста**\n",
    "   - Ограничьте длину текста (например, до 300 символов), чтобы избежать избыточности.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Как предобработка текста может улучшить модель?**\n",
    "\n",
    "- **Уменьшение шума**: сокращение \"мусора\" в данных позволяет модели лучше понимать важные паттерны.\n",
    "- **Повышение информативности**: лемматизация и удаление стоп-слов позволяет сконцентрироваться на значимых словах.\n",
    "- **Снижение размерности**: меньшее количество уникальных токенов делает обучение быстрее и точнее.\n",
    "- **Улучшение обобщающей способности**: унификация текста (например, приведение к одному регистру или удаление синонимов) помогает модели обобщать и лучше прогнозировать.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Итоговая рекомендация:**\n",
    "\n",
    "- Если текстовые данные \"чистые\" и небольшие по объему, можно попробовать оставить их без обработки и положиться на встроенные возможности CatBoost.\n",
    "- Если данные \"шумные\" или объемны, используйте базовую предобработку:\n",
    "  - Удалите спецсимволы, дубликаты, ненужные пробелы и привидите текст к единому формату.\n",
    "  - Опционально: примените лемматизацию или выделите ключевые слова.\n",
    "- Всегда проверяйте качество модели с обработанным и необработанным текстом, чтобы понять, что работает лучше.\n",
    "\n",
    "#### Удалять числовые значения из текста объявления о продаже квартир **не всегда целесообразно**, так как числа могут содержать важную информацию, например: \n",
    "\n",
    "- Метраж квартиры.\n",
    "- Номер этажа.\n",
    "- Год постройки.\n",
    "- Контактные данные (телефоны можно удалить, если они не нужны).\n",
    "- Цены или скидки.\n",
    "\n",
    "### Рекомендация:\n",
    "Удаляйте только те числовые значения, которые однозначно являются \"мусором\" (например, телефонные номера, случайные цифры) или не относятся к объявлению. Если числа могут быть важными признаками (например, этаж или метраж), оставляйте их.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ebd60e-2d9c-4c44-ae6f-2a457f824a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "mystem = Mystem()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Удаление спецсимволов и приведение к нижнему регистру\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    # Токенизация\n",
    "    tokens = word_tokenize(text)\n",
    "    # Удаление стоп-слов\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Лемматизация (опционально)\n",
    "    tokens = [mystem.lemmatize(word)[0] for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Пример\n",
    "sample_text = \"Шикарная квартира в центре города!!! Продается срочно!!!\"\n",
    "cleaned_text = preprocess_text(sample_text)\n",
    "print(cleaned_text)  # \"шикарный квартира центр город продаваться срочно\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f15f0ff-5713-4804-993e-d750d62c1d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# удаление цифр\n",
    "\n",
    "import re\n",
    "\n",
    "#re.sub(r'[^\\w\\s]+|[\\d]+', r'',\"112 sadd dfdf 21 dfheif 12,23 12\").strip()\n",
    "\n",
    "text = re.sub(r\"\\d+\", \"\", text, flags = re.UNICODE)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee12367c-438e-436c-af67-780a08c48068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# токенизация\n",
    "tokens = word_tokenize(text.lower())\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fcc2fa-3c46-4f56-81b1-721a7313bc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление пунктуационных знаков и прочих символов\n",
    "punctuation_marks = ['«', '»', '№', '!', ',', '(', ')', ':', ';', '-', '—', '?', '.', '..', '...', '€', '₽', '%', '#']\n",
    "\n",
    "only_words = []\n",
    "\n",
    "for token in tokens:\n",
    "    if token not in punctuation_marks:\n",
    "        only_words.append(token)\n",
    "        \n",
    "only_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585a11c3-d471-4a00-bc7c-e32cb326ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Приводим слова к нормальной форме\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "lemmas = []\n",
    "for token in only_words:\n",
    "    lemmas.append(morph.parse(token)[0].normal_form)\n",
    "\n",
    "lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a03b4-9998-4cba-ab13-807d0ff839dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление стоп слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5810233b-fbbd-486f-82b2-0069b6d301ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\skapr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35ee8925-35f9-4fa2-81e3-2fe4aeff946e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['и',\n",
       " 'в',\n",
       " 'во',\n",
       " 'не',\n",
       " 'что',\n",
       " 'он',\n",
       " 'на',\n",
       " 'я',\n",
       " 'с',\n",
       " 'со',\n",
       " 'как',\n",
       " 'а',\n",
       " 'то',\n",
       " 'все',\n",
       " 'она',\n",
       " 'так',\n",
       " 'его',\n",
       " 'но',\n",
       " 'да',\n",
       " 'ты',\n",
       " 'к',\n",
       " 'у',\n",
       " 'же',\n",
       " 'вы',\n",
       " 'за',\n",
       " 'бы',\n",
       " 'по',\n",
       " 'только',\n",
       " 'ее',\n",
       " 'мне',\n",
       " 'было',\n",
       " 'вот',\n",
       " 'от',\n",
       " 'меня',\n",
       " 'еще',\n",
       " 'нет',\n",
       " 'о',\n",
       " 'из',\n",
       " 'ему',\n",
       " 'теперь',\n",
       " 'когда',\n",
       " 'даже',\n",
       " 'ну',\n",
       " 'вдруг',\n",
       " 'ли',\n",
       " 'если',\n",
       " 'уже',\n",
       " 'или',\n",
       " 'ни',\n",
       " 'быть',\n",
       " 'был',\n",
       " 'него',\n",
       " 'до',\n",
       " 'вас',\n",
       " 'нибудь',\n",
       " 'опять',\n",
       " 'уж',\n",
       " 'вам',\n",
       " 'ведь',\n",
       " 'там',\n",
       " 'потом',\n",
       " 'себя',\n",
       " 'ничего',\n",
       " 'ей',\n",
       " 'может',\n",
       " 'они',\n",
       " 'тут',\n",
       " 'где',\n",
       " 'есть',\n",
       " 'надо',\n",
       " 'ней',\n",
       " 'для',\n",
       " 'мы',\n",
       " 'тебя',\n",
       " 'их',\n",
       " 'чем',\n",
       " 'была',\n",
       " 'сам',\n",
       " 'чтоб',\n",
       " 'без',\n",
       " 'будто',\n",
       " 'чего',\n",
       " 'раз',\n",
       " 'тоже',\n",
       " 'себе',\n",
       " 'под',\n",
       " 'будет',\n",
       " 'ж',\n",
       " 'тогда',\n",
       " 'кто',\n",
       " 'этот',\n",
       " 'того',\n",
       " 'потому',\n",
       " 'этого',\n",
       " 'какой',\n",
       " 'совсем',\n",
       " 'ним',\n",
       " 'здесь',\n",
       " 'этом',\n",
       " 'один',\n",
       " 'почти',\n",
       " 'мой',\n",
       " 'тем',\n",
       " 'чтобы',\n",
       " 'нее',\n",
       " 'сейчас',\n",
       " 'были',\n",
       " 'куда',\n",
       " 'зачем',\n",
       " 'всех',\n",
       " 'никогда',\n",
       " 'можно',\n",
       " 'при',\n",
       " 'наконец',\n",
       " 'два',\n",
       " 'об',\n",
       " 'другой',\n",
       " 'хоть',\n",
       " 'после',\n",
       " 'над',\n",
       " 'больше',\n",
       " 'тот',\n",
       " 'через',\n",
       " 'эти',\n",
       " 'нас',\n",
       " 'про',\n",
       " 'всего',\n",
       " 'них',\n",
       " 'какая',\n",
       " 'много',\n",
       " 'разве',\n",
       " 'три',\n",
       " 'эту',\n",
       " 'моя',\n",
       " 'впрочем',\n",
       " 'хорошо',\n",
       " 'свою',\n",
       " 'этой',\n",
       " 'перед',\n",
       " 'иногда',\n",
       " 'лучше',\n",
       " 'чуть',\n",
       " 'том',\n",
       " 'нельзя',\n",
       " 'такой',\n",
       " 'им',\n",
       " 'более',\n",
       " 'всегда',\n",
       " 'конечно',\n",
       " 'всю',\n",
       " 'между',\n",
       " 'кв.м',\n",
       " 'вы',\n",
       " 'кв.м.',\n",
       " 'м.',\n",
       " 'кв.',\n",
       " 'кв',\n",
       " 'м']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dop_words = ['кв.м', 'вы', 'кв.м.', 'м.', 'кв.', 'кв', 'м']\n",
    "\n",
    "stop_words = stopwords.words(\"russian\")\n",
    "stop_words = stop_words + dop_words\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd7aea06-428b-4f6f-a2d3-e23dffa40fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stop_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>и</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>в</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>во</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>не</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>что</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>кв.м.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>м.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>кв.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>кв</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>м</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    stop_words\n",
       "0            и\n",
       "1            в\n",
       "2           во\n",
       "3           не\n",
       "4          что\n",
       "..         ...\n",
       "153      кв.м.\n",
       "154         м.\n",
       "155        кв.\n",
       "156         кв\n",
       "157          м\n",
       "\n",
       "[158 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = pd.DataFrame(stop_words, columns = ['stop_words'])\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90a1842c-cc3e-4bcf-8cd6-e7ccc344420e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skapr\\AppData\\Local\\Temp\\ipykernel_13660\\312388849.py:3: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
      "  stop_words.to_excel(writer, 'stop_words_nltk')\n"
     ]
    }
   ],
   "source": [
    "#Сохраняем в Excel\n",
    "writer = pd.ExcelWriter('stop_words_nltk.xlsx')\n",
    "stop_words.to_excel(writer, 'stop_words_nltk')\n",
    "writer._save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f02405-a068-406b-b811-bae985c2bbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = []\n",
    "for token in lemmas:\n",
    "    if token not in stop_words:\n",
    "        filtered_words.append(token)\n",
    "\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c949f89-a41c-4588-8355-6ccc0705b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Полная предобработка текста\n",
    "def preprocess(text, stop_words, punctuation_marks, morph):\n",
    "    text = re.sub(r\"\\d+\", \"\", text, flags = re.UNICODE)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    preprocessed_text = []\n",
    "    for token in tokens:\n",
    "        if token not in punctuation_marks:\n",
    "            lemma = morph.parse(token)[0].normal_form\n",
    "            if lemma not in stop_words:\n",
    "                preprocessed_text.append(lemma)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb129065-4a79-432e-8c7d-2d1c201fe874",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t[0:5].apply(lambda row: preprocess(row['Текст'], punctuation_marks, stop_words, morph), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff617b14-40a0-4b58-9154-adac498a2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t['Preprocessed_texts'] = df_t.apply(lambda row: preprocess(row['Текст'], punctuation_marks, stop_words, morph), \n",
    "                                          axis=1)\n",
    "df_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcfd6e1-2f55-46b1-9f61-cf726f7fe15b",
   "metadata": {},
   "source": [
    "### **Другие подходы к лемматизации для больших объемов данных**\n",
    "\n",
    "1. **Использование библиотеки Natasha**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cc0742-2cd0-4d42-95ea-93e7e0b87e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "def preprocess_text_natasha(text):\n",
    "   # Приведение к нижнему регистру\n",
    "   text = text.lower()\n",
    "   # Создание объекта документа\n",
    "   doc = Doc(text)\n",
    "   doc.segment(segmenter)\n",
    "   for token in doc.tokens:\n",
    "       token.lemmatize(morph_vocab)\n",
    "   # Возврат лемм\n",
    "   return ' '.join([token.lemma for token in doc.tokens if token.lemma and not token.lemma.isdigit()])\n",
    "\n",
    "# Пример\n",
    "processed_text = preprocess_text_natasha(\"Продается шикарная квартира, 100 метров, 3 этаж, цена 5.5 млн!\")\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d98df6-c7cd-4154-8031-cf08dfb48fe1",
   "metadata": {},
   "source": [
    "2. **Использование `spaCy` с русским языковым пакетом**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fda2e7-cf67-4bcb-8532-dcb5d2c6d1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ru_core_news_md\")\n",
    "\n",
    "def preprocess_text_spacy(text):\n",
    "   # Обработка текста с spaCy\n",
    "   doc = nlp(text.lower())\n",
    "   return ' '.join([token.lemma_ for token in doc if not token.is_digit and token.is_alpha])\n",
    "\n",
    "# Пример\n",
    "processed_text = preprocess_text_spacy(\"Продается шикарная квартира, 100 метров, 3 этаж, цена 5.5 млн!\")\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfead4ec-a30a-430e-8b28-cce7350f061a",
   "metadata": {},
   "source": [
    "### Вывод:\n",
    "- Для больших выборок лучше использовать `Natasha` или `spaCy` — они быстрее.\n",
    "- Удаление чисел зависит от их значимости. Например:\n",
    "  - **Оставить числа**, если они содержат метраж или этаж.\n",
    "  - **Удалить числа**, если это телефонные номера или случайные цифры.\n",
    "\n",
    "Если нужно более точное управление обработкой текстов для повышения качества модели, пробуйте разные библиотеки и предобработки, сравнивая метрики на выходе."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96125012-a4ca-4c91-a0b5-5ff8d4b9c9b5",
   "metadata": {},
   "source": [
    "### CatBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b917a72-1284-4ba4-9be8-806b197a4a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# 1. Объявление модели с гиперпараметрами\n",
    "model = CatBoostClassifier(\n",
    "    iterations = 500,\n",
    "    depth = 6,\n",
    "    learning_rate = 0.1,\n",
    "    embedding_size = 20,       # Размер эмбеддингов\n",
    "    max_tokens = 2000,         # Максимальное количество токенов\n",
    "    ngrams = (1, 2, 3)         # Используем n-граммы\n",
    ")\n",
    "\n",
    "# 2. Обучение модели с указанием текстовых столбцов\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    text_features=['text_column']  # Указываем текстовые признаки\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a47f4f-faf8-4600-93ba-8b9f22aab0a5",
   "metadata": {},
   "source": [
    "### Гиперпараметры для обработки текстовых данных в CatBoost\n",
    "\n",
    "В CatBoost есть несколько гиперпараметров, которые касаются работы с текстовыми признаками:\n",
    "\n",
    "1. **`text_features`**:\n",
    "   - Это список, в котором указываются столбцы данных, содержащие текстовые признаки. Вы передаете текстовые данные через этот параметр.\n",
    "   \n",
    "2. **`embedding_size`**:\n",
    "   - Этот параметр задает размер эмбеддингов для текстовых признаков. Эмбеддинги — это числовые представления, которые CatBoost использует для текстовых данных. Чем больше размер эмбеддинга, тем больше информации может быть сохранено в этих представлениях, но это также увеличивает вычислительную нагрузку.\n",
    "   - **Значение по умолчанию**: 10.\n",
    "\n",
    "3. **`max_tokens`**:\n",
    "   - Этот параметр контролирует максимальное количество токенов, которые извлекаются из текста. Если текст содержит больше токенов, чем указано в `max_tokens`, то они будут отброшены. Это ограничивает количество данных, которые модель будет обрабатывать.\n",
    "   - **Значение по умолчанию**: 2000 токенов.\n",
    "\n",
    "4. **`tokenizer`**:\n",
    "   - Здесь можно выбрать тип токенизатора. CatBoost по умолчанию использует токенизатор на основе `mecab` или `spacy` в зависимости от конфигурации, но можно передать свою собственную функцию для токенизации.\n",
    "   \n",
    "5. **`ngrams`**:\n",
    "   - Этот параметр позволяет контролировать использование n-грамм. Вы можете задать, какие именно n-граммы должны использоваться в преобразовании текста (1-граммы, 2-граммы и т.д.). Это полезно для того, чтобы контролировать длину последовательностей слов, которые будут извлекаться и использоваться для обучения.\n",
    "\n",
    "### Как внести изменения?\n",
    "\n",
    "1. **Настройка длины n-грамм**:\n",
    "   Если у вас есть текст с важными фразами, которые могут быть представлены большими n-граммами (например, 2 или 3 слова), вы можете изменить параметр `ngrams`, чтобы модель учитывала такие фразы.\n",
    "\n",
    "2. **Размер эмбеддингов**:\n",
    "   Увеличив размер эмбеддингов (`embedding_size`), вы можете позволить модели лучше захватывать информацию о контексте, но это также потребует больше памяти и вычислительных ресурсов.\n",
    "\n",
    "3. **Ограничение количества токенов**:\n",
    "   Если ваши тексты очень длинные и слишком подробные, вы можете установить параметр `max_tokens`, чтобы ограничить количество информации, которое будет подано в модель.\n",
    "\n",
    "### Рекомендации:\n",
    "\n",
    "- **Токенизация**: Для большинства стандартных задач в недвижимости (например, объявления о продаже квартир) использование стандартного токенизатора CatBoost подходит, но если текст содержит специфическую терминологию или имеет особую структуру, можно поэкспериментировать с собственной функцией токенизации.\n",
    "- **Размер эмбеддингов**: Если у вас небольшие тексты и они не содержат слишком сложной информации, можно оставить эмбеддинги небольшими. Если текст содержит много уникальных терминов, увеличьте размер эмбеддингов, чтобы улучшить качество представления.\n",
    "- **Использование n-грамм**: Использование n-грамм, особенно 2-грамм и 3-грамм, может помочь захватить контекст и фразы, которые часто встречаются вместе. Например, фразы как \"метро рядом\" или \"новый ремонт\" могут дать важную информацию о стоимости квартиры."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58125dbe-a715-40b5-9d1b-85359574ca4a",
   "metadata": {},
   "source": [
    "### Для решения задачи обработки текста и улучшения модели CatBoost, давайте разберемся по шагам:\n",
    "\n",
    "### 1. **Какие спецсимволы (в т.ч. пунктуацию) лучше убрать?**\n",
    "   Спецсимволы и знаки препинания могут нести определенную информацию в контексте текста, но в большинстве случаев они не добавляют значимого контекста для модели. Особенно для модели, которая работает с текстами в контексте недвижимости, такие символы могут только замедлять обучение.\n",
    "\n",
    "   **Рекомендуется удалить следующие спецсимволы**:\n",
    "   - Все знаки препинания: `.`, `,`, `!`, `?`, `-`, `:`, `;`, `()`, `[]`, `{}`, `'`, `\"`, и другие неалфавитные символы.\n",
    "   - Символы, которые могут быть использованы для указания на цену или метры, такие как `€`, `₽`, `%`, `#`, и другие.\n",
    "   \n",
    "   **Примечание**: Если в тексте встречаются символы, которые могут быть важными, например, доллары или другие валюты (особенно если они используются в качестве единиц измерения), их можно оставить.\n",
    "\n",
    "### 2. **Убирать ли из текста числа/цифры?**\n",
    "   В контексте объявлений о продаже недвижимости цифры могут играть важную роль (например, указания на цену, площадь, этажность и другие параметры). Поэтому **не рекомендуется полностью удалять все числа**.\n",
    "\n",
    "   **Рекомендуется**:\n",
    "   - Заменить цифры на токены типа `NUMBER`, чтобы модель могла воспринимать их как категории. Это может помочь CatBoost обнаруживать взаимосвязь между цифрами и другими признаками.\n",
    "   - Например, \"квартира 45 м²\" может быть преобразована в \"квартира NUMBER м²\". Это сохранит информацию о числовых значениях, но позволит модели работать с ними как с категориальными признаками.\n",
    "\n",
    "### 3. **Какие дополнительные стоп-слова необходимо убрать (кроме имеющихся в nltk)?**\n",
    "   Стоп-слова — это слова, которые в большинстве случаев не несут значимой информации, такие как \"и\", \"в\", \"на\", \"с\", и так далее. Для текста объявлений важно не только использовать стандартный список стоп-слов (например, из `nltk`), но и добавить специфические слова, которые могут нести мало смысла в контексте задачи.\n",
    "\n",
    "   **Рекомендуется удалить следующие типы слов**:\n",
    "   - **Стоп-слова**: \"квартира\", \"продажа\", \"по\", \"с\", \"в\", \"это\", \"от\", \"на\", \"все\", \"по\", \"будет\", \"есть\" и т.д. Эти слова повторяются часто, но не добавляют значимой информации для классификации.\n",
    "   - **Часто встречающиеся слова, которые могут быть специфичными для недвижимости**, например, \"комнаты\", \"метры\", \"сделка\", \"состояние\", \"после ремонта\", если они не влияют на цену или другие важные признаки.\n",
    "\n",
    "   Для этого вы можете использовать **Word Frequency** или **TF-IDF** на основе обучающих данных, чтобы выявить дополнительные слова, которые повторяются, но не дают полезной информации. Слова, которые будут встречаться во всех объявлениях, также могут быть удалены.\n",
    "\n",
    "### 4. **Какой размер `embedding_size` лучше задать для данного текста?**\n",
    "   `embedding_size` определяет размер вектора, в который будут преобразованы текстовые данные. **Слишком маленькие значения** могут не уловить всей сложности текста, а **слишком большие** могут привести к переобучению или чрезмерным вычислительным затратам.\n",
    "\n",
    "   **Рекомендуется начинать с embedding_size от 10 до 100**. Для небольших наборов текста (например, с 50 000 строками, как у вас) разумным будет начинать с **20–50**.\n",
    "\n",
    "   - **Меньший размер** (например, 20) подойдет, если текст не сильно разнообразен и можно обобщить информацию.\n",
    "   - **Больший размер** (например, 50 или 100) подойдет, если текст разнообразен и имеет много уникальных фраз или ключевых слов.\n",
    "\n",
    "### 5. **Какое максимальное количество токенов (`max_tokens`) лучше задать для данного текста?**\n",
    "   `max_tokens` ограничивает количество уникальных токенов (слов или символов), которые могут быть обработаны моделью. Если этот параметр слишком маленький, часть информации может быть проигнорирована.\n",
    "\n",
    "   **Рекомендуется начать с max_tokens = 5000–10000**. Это число обычно достаточное для обработки даже больших объемов текста (например, 50 000 строк), при этом оно позволяет улавливать множество уникальных токенов, сохраняя при этом управляемый размер модели.\n",
    "\n",
    "   Важно отметить, что если текст очень длинный (например, описание с большим количеством фраз), можно будет использовать срезы текста или сокращать длину вхождений (например, по важнейшим словам).\n",
    "\n",
    "### 6. **Какую длину последовательностей слов (`ngrams`) лучше задать для данного текста?**\n",
    "   `ngrams` определяет, какие последовательности слов будут использоваться для создания признаков. В случае с недвижимостью полезны могут быть как **биграммы (2 слова)**, так и **триграммы (3 слова)**, так как они помогают лучше уловить контекст.\n",
    "\n",
    "   **Рекомендуется начать с ngrams=(1, 2, 3)**. Это даст модели возможность использовать как одиночные слова, так и их пары и тройки, что позволит захватить контекст в описаниях.\n",
    "\n",
    "   Если же текст более специфичен и не содержит много контекстных фраз, можно ограничиться **(1, 2)**.\n",
    "\n",
    "### 7. **Дополнительные рекомендации для улучшения качества модели:**\n",
    "   - **Лемматизация**: Для работы с текстами недвижимости лемматизация может помочь сократить разнообразие слов и улучшить обобщение модели. Используйте такие инструменты, как `pymystem2` или `spacy` для лемматизации.\n",
    "   - **Чистка текста**: Убедитесь, что вы удаляете ненужные символы, ссылки, эмодзи, лишние пробелы и прочее.\n",
    "   - **Предобработка категориальных данных**: Применяйте one-hot encoding или other encoding для категориальных признаков, таких как \"район\", \"состояние\" и другие.\n",
    "   - **Использование частотных признаков**: После применения text features вы можете анализировать важность признаков и дополнительно фильтровать незначимые токены или фразы с низкой важностью.\n",
    "\n",
    "### Заключение:\n",
    "Ваши параметры, такие как `embedding_size`, `ngrams`, `max_tokens`, должны быть выбраны в зависимости от объема и специфики данных. Чтобы улучшить модель, рекомендуется тщательно проанализировать текст, удалить избыточные или бессмысленные слова, а также настроить параметры, которые оптимально подходят для данных в недвижимости."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b10840-f225-4834-98f7-ba48a6b9404a",
   "metadata": {},
   "source": [
    "### Для решения нашей задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7894af72-1ee1-4d97-8953-69f273c64069",
   "metadata": {},
   "source": [
    "### 1. Какие спецсимволы (включая пунктуацию) лучше убрать? \n",
    "- Рекомендуется убрать:   \n",
    "  - Спецсимволы, не влияющие на смысл: !@#$%^&*()_+={}[]|:;'\"<>,/?\\`~.   \n",
    "  - Повторяющиеся пробелы.   \n",
    " \n",
    "- Оставить:   \n",
    "  - Разделители, влияющие на структуру текста: ., ,, %, №, -.   \n",
    "    Они могут быть важны для выделения чисел, адресов или обозначений (например, \"№4\", \"кухня 10,6 кв. м\").   \n",
    " \n",
    "--- \n",
    " \n",
    "### 2. Убирать ли числа/цифры? \n",
    "- Не убирать полностью, так как:   \n",
    "  - Цифры указывают на характеристики недвижимости (площадь, этажность, год постройки, стоимость).   \n",
    "  - Могут использоваться для предсказаний модели (например, связь цены и площади).   \n",
    " \n",
    "- Рекомендация:   \n",
    "  - Нормализовать числа: конвертировать их в стандартный формат (например, 65,5 → `65.5`), чтобы избежать ошибок токенизации.   \n",
    " \n",
    "--- \n",
    " \n",
    "### 3. Какие дополнительные стоп-слова нужно убрать? \n",
    "- К стандартным стоп-словам из nltk добавьте:   \n",
    "  - Слова, характерные для всех объявлений, которые не несут полезной информации:   \n",
    "    - \"продается\", \"квартира\", \"студия\", \"жк\", \"продам\", \"собственник\", \"риелтор\", \"без посредников\".   \n",
    "    - Примеры шаблонных фраз: \"агентам не беспокоить\", \"дешевле чем у застройщика\".   \n",
    " \n",
    "- Создайте список по ключевым часто встречающимся словам (анализировать можно через частотный словарь).   \n",
    " \n",
    "--- \n",
    " \n",
    "### 4. Какой размер embedding_size лучше задать? \n",
    "- Для текстов с такими характеристиками и размером датасета (`24096` строк):   \n",
    "  - Рекомендация: начните с embedding_size = 100 или 150.   \n",
    "  - Если увеличите объем данных или добавите внешние данные, можно поднять до 300.   \n",
    " \n",
    "--- \n",
    " \n",
    "### 5. Какое максимальное количество токенов max_tokens лучше задать? \n",
    "- Средняя длина объявлений (по первым строкам) составляет около 150–200 слов.   \n",
    "- Рекомендуемое значение для max_tokens:   \n",
    "  - 250–300, чтобы не терять длинные описания.   \n",
    "  - Укоротите тексты при необходимости, чтобы исключить нерелевантную информацию.   \n",
    " \n",
    "--- \n",
    " \n",
    "### 6. Какую длину последовательностей слов ngrams лучше задать? \n",
    "- Для текста объявлений хорошо работают:   \n",
    "  - Unigrams и bigrams: выявляют отдельные слова и простые фразы.   \n",
    "  - Trigrams (ограниченно): полезны для выявления устойчивых фраз (например, \"кухня 10 кв\").   \n",
    " \n",
    "--- \n",
    " \n",
    "### Дополнительные рекомендации: \n",
    "1. Очистка текста: \n",
    "   - Приведите текст к нижнему регистру. \n",
    "   - Уберите лишние пробелы.   \n",
    " \n",
    "2. Лемматизация или стемминг: \n",
    "   - Лемматизируйте текст с помощью библиотеки (`pymorphy2` или `spacy`) для русскоязычных данных.   \n",
    " \n",
    "3. Частотный анализ: \n",
    "   - Постройте частотный словарь, чтобы понять, какие слова слишком часты или редки (редкие можно удалить).   \n",
    " \n",
    "4. Сохранение доменных знаний: \n",
    "   - Убедитесь, что важные термины, такие как \"кухня\", \"ванна\", \"сплит-система\", остаются в тексте после очистки.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8c17d7-aa10-4a3b-87e5-b52e4308f0ef",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93018467-c5ab-4bea-87eb-20149b64a66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Текст</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Продам 3-комнатную квартиру с Евроремонтом, 65...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Собственник! Продаю без посредников, Квартира ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Агентам не Беспокоить. Дешевле чем от застройщ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Продажа от собственника, никаких риелтор них к...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Продаётся уютная студия в ЖК« Светлоград» от с...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24091</th>\n",
       "      <td>Продам 2к квартиру в доме комфорт класса в мал...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24092</th>\n",
       "      <td>Отличное расположение и самодостаточная инфрас...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24093</th>\n",
       "      <td>Продаётся 2-комнатная квартира в строящемся до...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24094</th>\n",
       "      <td>Toт самый ЖК рядoм с Паркоm Галицkогo! Парково...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24095</th>\n",
       "      <td>Продаётся 1-комнатная квартира в строящемся до...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24096 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Текст\n",
       "0      Продам 3-комнатную квартиру с Евроремонтом, 65...\n",
       "1      Собственник! Продаю без посредников, Квартира ...\n",
       "2      Агентам не Беспокоить. Дешевле чем от застройщ...\n",
       "3      Продажа от собственника, никаких риелтор них к...\n",
       "4      Продаётся уютная студия в ЖК« Светлоград» от с...\n",
       "...                                                  ...\n",
       "24091  Продам 2к квартиру в доме комфорт класса в мал...\n",
       "24092  Отличное расположение и самодостаточная инфрас...\n",
       "24093  Продаётся 2-комнатная квартира в строящемся до...\n",
       "24094  Toт самый ЖК рядoм с Паркоm Галицkогo! Парково...\n",
       "24095  Продаётся 1-комнатная квартира в строящемся до...\n",
       "\n",
       "[24096 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_file = pd.read_excel(\"текст.xlsx\")\n",
    "read_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "621f68cd-74a0-4863-b775-8a793145caef",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file.to_csv (\"текст.csv\",  \n",
    "                  index = False, \n",
    "                  sep =\",\", \n",
    "                  #encoding = \"utf-8\"\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c976ddd6-ce1a-4b21-a15d-17d33a1cb78d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Текст</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Продам 3-комнатную квартиру с Евроремонтом, 65...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Собственник! Продаю без посредников, Квартира ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Агентам не Беспокоить. Дешевле чем от застройщ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Продажа от собственника, никаких риелтор них к...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Продаётся уютная студия в ЖК« Светлоград» от с...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24091</th>\n",
       "      <td>Продам 2к квартиру в доме комфорт класса в мал...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24092</th>\n",
       "      <td>Отличное расположение и самодостаточная инфрас...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24093</th>\n",
       "      <td>Продаётся 2-комнатная квартира в строящемся до...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24094</th>\n",
       "      <td>Toт самый ЖК рядoм с Паркоm Галицkогo! Парково...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24095</th>\n",
       "      <td>Продаётся 1-комнатная квартира в строящемся до...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24096 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Текст\n",
       "0      Продам 3-комнатную квартиру с Евроремонтом, 65...\n",
       "1      Собственник! Продаю без посредников, Квартира ...\n",
       "2      Агентам не Беспокоить. Дешевле чем от застройщ...\n",
       "3      Продажа от собственника, никаких риелтор них к...\n",
       "4      Продаётся уютная студия в ЖК« Светлоград» от с...\n",
       "...                                                  ...\n",
       "24091  Продам 2к квартиру в доме комфорт класса в мал...\n",
       "24092  Отличное расположение и самодостаточная инфрас...\n",
       "24093  Продаётся 2-комнатная квартира в строящемся до...\n",
       "24094  Toт самый ЖК рядoм с Паркоm Галицkогo! Парково...\n",
       "24095  Продаётся 1-комнатная квартира в строящемся до...\n",
       "\n",
       "[24096 rows x 1 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text = pd.read_csv(\"текст.csv\", sep = \"\\t\", encoding = \"utf-8\")\n",
    "df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e2d5a4-442e-4e0b-bb5e-91c2bd2032c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
